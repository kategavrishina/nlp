{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение языка и VK API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании вам нужно будет:\n",
    "\n",
    "* используя API Вконтакте, скачать комментарии к первым ста постам из пяти сообществ\n",
    "* натренировать модель распознавания языков на статьях из Википедии.\n",
    "* распознать язык всех комментариев, где в тексте есть 10 и более символов, и построить статистику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VK API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подключения к ВКонтакте мы будем использовать VK API. Здесь есть документация к этой библиотеке https://vk-api.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install vk_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import vk_api\n",
    "\n",
    "# здесь нужно ввести данные своего аккаунта\n",
    "# когда будете сдавать, не забудьте убрать эти две строчки из ноутбука\n",
    "user = '' # вставьте сюда свой номер телефона\n",
    "password = '' # вставьте сюда свой пароль\n",
    "\n",
    "# авторизация\n",
    "vk_session = vk_api.VkApi(login=user, password=password)\n",
    "vk_session.auth()\n",
    "\n",
    "vk = vk_session.get_api() # объект с API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить записи со страницы можно с помощью метода `wall.get`. Он принимает параметр `domain` — короткое имя пользователя или сообщества — и `count` — количество записей, которое вы хотите получить (максимум — 100). По Список методов для работы со стенами: https://vk.com/dev/wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, вот так можно получить последние две записи с вот этой страницы https://vk.com/futureisnow. Выдача представляет собой словарь, в котором в поле `items` записан список словарей, содержащий информацию о каждой из записи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vk.wall.get(domain=\"futureisnow\", count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью метода `groups.getById` можно получить информацию о сообществе, в том числе его id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk.groups.getById(group_ids=\"futureisnow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте информацию о последних ста записях в следующих пабликах: https://vk.com/futureisnow, https://vk.com/eternalclassic, https://vk.com/ukrlit_memes, https://vk.com/ukrainer_net, https://vk.com/amanzohel, https://vk.com/barg_kurumk_culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список domain'ов, чтобы вам не копировать их самими :)\n",
    "publics = [\"futureisnow\",\n",
    "           \"eternalclassic\",\n",
    "           \"ukrlit_memes\",\n",
    "           \"ukrainer_net\",\n",
    "           \"amanzohel\",\n",
    "           \"barg_kurumk_culture\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publics_ids = {}\n",
    "\n",
    "for i in vk.groups.getById(group_ids=publics):\n",
    "    publics_ids[i['screen_name']] = -i['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publics = publics_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = {} # ключи — это паблики\n",
    "\n",
    "for i in publics.keys():\n",
    "    all_info = vk.wall.get(domain=i, count=100)\n",
    "    items[i] = all_info['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = []\n",
    "\n",
    "for i in items:\n",
    "    for each_post in items[i]:\n",
    "        all_posts.append([each_post['owner_id'], each_post['id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите в документации (https://vk.com/dev/wall) метод для получения комментариев и получите первые сто комментариев каждого поста из выборки для каждого паблика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = {}\n",
    "\n",
    "for i in all_posts:\n",
    "    infos[i[0], i[1]] = vk.wall.getComments(owner_id=i[0],\n",
    "                                            post_id=i[1], count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {}\n",
    "\n",
    "fut = []\n",
    "ete = []\n",
    "ukm = []\n",
    "ukn = []\n",
    "ama = []\n",
    "barg = []\n",
    "\n",
    "for i in infos:\n",
    "    for j in infos[i]['items']:\n",
    "        try:\n",
    "            if len(j['text']) > 10:\n",
    "                if j['owner_id'] == publics['futureisnow']:\n",
    "                    fut.append(j['text'])\n",
    "                elif j['owner_id'] == publics['eternalclassic']:\n",
    "                    ete.append(j['text'])\n",
    "                elif j['owner_id'] == publics['ukrlit_memes']:\n",
    "                    ukm.append(j['text'])\n",
    "                elif j['owner_id'] == publics['ukrainer_net']:\n",
    "                    ukn.append(j['text'])\n",
    "                elif j['owner_id'] == publics['amanzohel']:\n",
    "                    ama.append(j['text'])\n",
    "                elif j['owner_id'] == publics['barg_kurumk_culture']:\n",
    "                    barg.append(j['text'])\n",
    "        except:\n",
    "            continue\n",
    "corpora['futureisnow'] = fut\n",
    "corpora['eternalclassic'] = ete\n",
    "corpora['ukrlit_memes'] = ukm\n",
    "corpora['ukrainer_net'] = ukn\n",
    "corpora['amanzohel'] = ama\n",
    "corpora['barg_kurumk_culture'] = barg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '!@#$%^&*()_—-=+~:;`\\ \"\\'/?«».>,<'\n",
    "\n",
    "clear_corpora = {}\n",
    "\n",
    "for i in corpora:\n",
    "    clear_comments = []\n",
    "    for j in corpora[i]:\n",
    "        clear_comment = []\n",
    "        parts = nltk.word_tokenize(j.lower())\n",
    "        clear_parts = []\n",
    "        for part in parts:\n",
    "            clear_parts.append(part.strip(punc))\n",
    "        clear_comment = ' '.join(clear_parts)\n",
    "        if len(clear_comment) > 15:\n",
    "            clear_comments.append(clear_comment.strip())\n",
    "    clear_corpora[i] = clear_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание со звездочкой:** вы могли заметить, что если обращаться к каждому посту отдельно, то все занимает довольно продолжительное время (около пяти минут). Найдите в документации vk_api способ сделать это быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {}\n",
    "\n",
    "# ваш улучшенный код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наших комментариях встречались русский, украинский, английский и бурятский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = {'ru', 'uk', 'en', 'bxr'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте документы, на которых вы будете обучать свои модели. Для наших целей хорошо иметь для каждого языка корпус размером около 50 статей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang):\n",
    "    wiki_content = []\n",
    "    wikipedia.set_lang(lang)\n",
    "    pages = wikipedia.random(60)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "            wiki_content.append(\"%s\\n%s\" % (page.title, \n",
    "                                            page.content.replace('=', '')))\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print(\"Skip %s\" % page_name)\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_texts = {}\n",
    "for lang in langs:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте определялку на частотах слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_freqlist(wiki_pages, max_len=100):\n",
    "    freqlist = Counter()\n",
    "    for text in wiki_pages:\n",
    "        for word in nltk.word_tokenize(text.lower()):\n",
    "            if word.isalpha():\n",
    "                freqlist[word] += 1\n",
    "    return dict(freqlist.most_common(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_lists = {}\n",
    "for lang in langs:\n",
    "    freq_lists[lang] = collect_freqlist(wiki_texts[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lang_detect(freq_lists, text):\n",
    "    counts = Counter()\n",
    "    for lang, freq_list in freq_lists.items():\n",
    "        freq_list = Counter(freq_list)\n",
    "        for word in nltk.word_tokenize(text):\n",
    "            counts[lang] += int(freq_list[word] > 0)\n",
    "    return counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте определялку на символьных энграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1, 5),\n",
    "                                                     analyzer='char')\n",
    "vectorizer.fit(wiki_texts['ru'])\n",
    "for item in vectorizer.get_feature_names()[:100]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn import naive_bayes\n",
    "import numpy as np\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline.Pipeline([\n",
    "    ('vctr', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                                     analyzer='char')),\n",
    "    ('clf', naive_bayes.MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "lang_indices = []\n",
    "for lang in wiki_texts:\n",
    "    all_texts.extend(wiki_texts[lang])\n",
    "    lang_indices.extend([lang]*len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(np.array(all_texts), np.array(lang_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определите язык каждого комментария в каждом паблике с помощью определялки на частотах слов и покажите доли языков среди комментариев для каждого паблика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_detects_freqs = {}\n",
    "\n",
    "for i in clear_corpora:\n",
    "    new_list = []\n",
    "    for j in clear_corpora[i]:\n",
    "        try:\n",
    "            result = simple_lang_detect(freq_lists, j)\n",
    "            if result[0][1] > 0:\n",
    "                new_list.append(result[0][0])\n",
    "            else:\n",
    "                new_list.append('err')\n",
    "        except IndexError:\n",
    "            continue\n",
    "    lang_detects_freqs[i] = Counter(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = {}\n",
    "\n",
    "for a, b in lang_detects_freqs.items():\n",
    "    ldf[a] = {}\n",
    "    \n",
    "for key, value in lang_detects_freqs.items():\n",
    "    if 'ru' in value:\n",
    "        ldf[key]['русский'] = lang_detects_freqs[key]['ru']\n",
    "    if 'uk' in value:\n",
    "        ldf[key]['украинский'] = lang_detects_freqs[key]['uk']\n",
    "    if 'en' in value:\n",
    "        ldf[key]['английский'] = lang_detects_freqs[key]['en']\n",
    "    if 'err' in value:\n",
    "        ldf[key]['ошибка'] = lang_detects_freqs[key]['err']\n",
    "    if 'bxr' in value:\n",
    "        ldf[key]['бурятский'] = lang_detects_freqs[key]['bxr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ldf:\n",
    "    print('\\nВ паблике ', i, ':', sep='')\n",
    "    sum_all = 0\n",
    "    for j in ldf[i]:\n",
    "        sum_all += ldf[i][j]\n",
    "    for j in sorted(ldf[i], key=ldf[i].get, reverse=True):\n",
    "        print(j, ' составляет ', round(ldf[i][j]/sum_all, 3), '% (', ldf[i][j], ')', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте то же самое для определителя на символьных энграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_detects_ngrams = {}\n",
    "\n",
    "for i in new_corpora:\n",
    "    lang_detects_ngrams[i] = Counter(clf.predict(new_corpora[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldn = {}\n",
    "\n",
    "for a, b in lang_detects_ngrams.items():\n",
    "    ldn[a] = {}\n",
    "    \n",
    "for key, value in lang_detects_ngrams.items():\n",
    "    if 'ru' in value:\n",
    "        ldn[key]['русский'] = lang_detects_ngrams[key]['ru']\n",
    "    if 'uk' in value:\n",
    "        ldn[key]['украинский'] = lang_detects_ngrams[key]['uk']\n",
    "    if 'en' in value:\n",
    "        ldn[key]['английский'] = lang_detects_ngrams[key]['en']\n",
    "    if 'err' in value:\n",
    "        ldn[key]['ошибка'] = lang_detects_ngrams[key]['err']\n",
    "    if 'bxr' in value:\n",
    "        ldn[key]['бурятский'] = lang_detects_ngrams[key]['bxr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ldn:\n",
    "    print('\\nВ паблике ', i, ':', sep='')\n",
    "    sum_all = 0\n",
    "    for j in ldn[i]:\n",
    "        sum_all += ldn[i][j]\n",
    "    for j in sorted(ldn[i], key=ldn[i].get, reverse=True):\n",
    "        print(j, ' составляет ', round(ldn[i][j]/sum_all, 3), '% (', ldn[i][j], ')', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обсудите работу каждого из классификаторов, обсудите ошибки, объясните разницу в результатах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частотные слова:\n",
    "Очень радует соотношение яызков в русском и английском пабликах, так как оно, видимо, близко к реальности. Для украинских, к сожалению, классификатор сильно путается между русским и ураинским, наверно, из-за сходства языков и того, что, возможно, в этих пабликах многие пишут и по-русски тоже. По крайней мере, в ukrlit_memes, русскоязычных комментариев больше, чем в ukrainer_net. С бурятским у алгоритма все совсем плохо, слишком большой процент нераспознанных комментариев и, скорее всего, бурятских, принятых за русские. Но в этих пабликах тоже, видимо, много действительно русской речи. В целом, алгоритм работает неплохо, но можно и лучше, как мы видим из второго классификатора.\n",
    "\n",
    "\n",
    "N-grams:\n",
    "Радует уже то, что этот классификатор не может не распознать язык, он его все равно определяет к какому-то классу. Гораздо лучше выглядит статистика для украинских пабликов и для бурятских. Все равно присутствуют какие-то сомнительные \"бурятские\" комментарии во всех пабликах, а также какой-то процент \"английских\" в украинских и бурятских.\n",
    "\n",
    "\n",
    "В целом:\n",
    "Второй классификатор кажется гораздо более надежным. Скорее всего, это связано с тем, что комментарии достаточно короткие (как бы мы их не очищали), а также зачастую состоят из довольно несвойственных для википедии слов (какие-нибудь эмоциональные \"лол\" или ругательства, присущие каждому языку). Довольно странно судить по частотным для полуофициального источника словам комментарии в соц сети. Может, просто стоило взять частотный список подлиннее. N-граммы справились с задачей получше, но по небольшим ошибкам видно, что в алгоритме все же были какие-то \"выбросы\", возможно, связанные с неподходящей степенью чистки корпуса (смайлики, например). Но во втором классификаторе такие ошибки составляют слшком маленькю долю в соотношении, так что их можно списывать на помехи (или там действительно есть комментарии на соответствующих языках). Надежнее было бы проверить точность нашего алгоритма на какой-то выборке комментариев с уже определенным языком, с ответом, но у нас нет такой возможности."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
